<!doctype html>
<html lang="en">

    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
        <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" 
                               integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">

        <title>Who's Waldo?</title>

		<style>
			.card {
				position: relative;
				display: inline;
			}
			.card .img-top {
				position: absolute;
				top: 0;
				left: 0;
				opacity: 0;
			}
			.card:hover .img-top {
				opacity: 1;
			}
            .tab { margin-left: 40px; }
			.card2 {
				position: relative; width: 100%;height: 100%;
				display: inline;
			}
			.card2 .img-top {
				width:100%; height:100%;
				position: relative;
				top: 0;
				left: 0;
				opacity: 0.5;
			}
			.card2:hover .img-top {
				opacity: 1;
			}
		</style>
    </head>

    <body class="container" style="max-width:840px">

        <!-- Heading -->
        <div>
            <!-- title -->
            <div class='row mt-5 mb-3'>
                <div class='col text-center'>
                    <p class="h2 font-weight-normal">Who's Waldo?</p>
					<p class="h2 font-weight-normal">Linking People Across Text and Images</p>
                </div>
            </div>

            <!-- authors -->
            <div class="row text-center h6 font-weight-bold mb-2 ">
                <span><a class="col-md-4 col-xs-6 pb-2" href="https://www.linkedin.com/in/claire-yuqing-cui/">Claire Yuqing Cui *</a></span>
				<span><a class="col-md-4 col-xs-6 pb-2" href="https://apoorvkh.com">Apoorv Khandelwal *</a></span>
                <span><a class="col-md-4 col-xs-6 pb-2" href="https://yoavartzi.com/">Yoav Artzi</a></span>
                <span><a class="col-md-4 col-xs-6 pb-2" href="http://snavely.io/">Noah Snavely</a></span>
                <span><a class="col-md-4 col-xs-6 pb-2" href="https://www.cs.cornell.edu/~hadarelor/">Hadar Averbuch-Elor</a></span>
            </div>

            <!-- affiliations -->
            <div class='row mb-3'>
                <div class='col text-center'>
                    <p class="h6">
                        <a href="https://www.cornell.edu/"><span>Cornell University</span></a>
                        &
                        <a href="https://tech.cornell.edu/"><span>Cornell Tech</span></a>
                    </p>
                </div>
            </div>

            <!-- venue -->
            <div class='row mb-4'>
                <div class='col text-center'>
                    <p class="h6">Oral Presentation @ ICCV 2021</p>
                </div>
            </div>
        </div>


		<!-- teaser -->
		<div class='row justify-content-center'>

			<div class="card" style="width:450px">
                <img src="assets/teaser_1.jpg" class="img-fluid rounded mx-auto d-block" >
                <div class="img-top"><img src="assets/teaser_1_boxes.jpg" class="img-fluid rounded mx-auto d-block" ></div>
                <p style="margin: 10px"><mark style="font-weight: bold; background-color: #FF6347; color: black;">Sam Schulz</mark> passes to <mark style="font-weight: bold; background-color: #FFD701; color: black;">Curtly Hampton</mark> during the UWS Giants vs Eastlake NEAFL match at the Robertson Oval on 1 August 2015.</p>
            </div>
            <div class="card" style="width:380px">
				<img src="assets/teaser_2.jpg" class="img-fluid rounded mx-auto d-block" >
				<div class="img-top"><img src="assets/teaser_2_boxes.jpg" class="img-fluid rounded mx-auto d-block" ></div>
                <p style="margin: 5px"><mark style="font-weight: bold; background-color: #78CDAA; color: black;">Justyna Kowalczyk</mark>, <mark style="font-weight: bold; background-color: #FFB6C1; color: black;">Kikkan Randall</mark> and <mark style="font-weight: bold; background-color: #FFD701; color: black;">Ingvild Flugstad Østberg</mark> at the Royal Palace Sprint, part of the FIS World Cup 2012/2013, in Stockholm on March 20, 2013. <mark style="font-weight: bold; background-color: #FFB6C1; color: black;">Kikkan Randall</mark> won the sprint cup.</p>
            </div>

            <div class='text-center col-md-12 col-sm-12 col-xs-12 align-middle mt-1'>
                <p class='h6'>
                    <em>You may not recognize them, but can you identify the people in these captions?<br>(Hover over images to see answers!)</em>
                </p>
            </div>
			<div class='col-md-12 col-sm-12 col-xs-12 align-middle mt-1'>
                <p class="text-break">
                Above we show two image–caption pairs capturing interactions between people. Possible cues revealing the correspondence between names in the captions and people in the images include:
                    <p class="tab">
                    (i) the action between two people ("Sam <em>passes to</em> Curtly</i>"),<br>
                    (ii) "Kikkan Randall won the sprint cup", so she is the one holding the trophy, and<br>
                    (iii) there happens to be a left-to-right ordering of people in the image on the right—this happens frequently in real-life image–caption pairs.
                    </p>
                </p>
			</div>
            <div class='col-md-12 col-sm-12 col-xs-12 align-middle mt-1'>
                <p class='h6 font-weight-bold '>
                In this work, we present an new task, <em>person-centric visual grounding</em>, which features the challenge above. We also provide a benchmark dataset of 270K image-caption pairs and propose a Transformer-based method for this task.
                </p>
			</div>
		</div>


        <!-- Paper section -->
        <div>
            <hr>
            <div class='row'>
                <div class='col-md-3 col-sm-3 col-xs-12 text-center col-sm-3'>
                    <div class="row mt-4">
                        <a href="https://arxiv.org/pdf/" style="max-width:200px; margin-left:auto; margin-right:auto"><!-- pdf link -->
                            <img src="assets/paper-snapshot.png" alt="paper-snapshot" class="img-thumbnail" width="80%" style="box-shadow: 10px 10px 5px grey;">
                        </a>
                    </div>
                    <div class="row mt-4">
                        <div class="col">
                            <a class="h5" href="https://arxiv.org/abs/" style="margin-right:10px"><span>[Arxiv]</span></a>
                            <a class="h5" href="https://github.com/apoorvkh/whos-waldo" style="margin-right:10px">
                                <span>[Code] (coming soon)</span>
                            </a>
                        </div>
                    </div>
                </div>
                <div class='col-md-9 col-sm-9 col-xs-12'>
                    <p class='h4 font-weight-bold '>Abstract</p>
                    <p style='line-height:1;'>
                    We present a task and benchmark dataset for <b>person-centric visual grounding</b>, the problem of linking between people named in a caption and people pictured in an image. In contrast to prior work in visual grounding, which is predominantly object-based, our new task masks out the names of people in captions in order to encourage methods trained on such image–caption pairs to focus on contextual cues (such as rich interactions between multiple people), rather than learning associations between names and appearances. To facilitate this task, we introduce a new dataset, <b>Who's Waldo</b>, mined automatically from image–caption data on Wikimedia Commons. We propose a Transformer-based method that outperforms several strong baselines on this task, and are releasing our data to the research community to spur work on contextual models that consider both vision and language.
                    </p>
                </div>
            </div>
        </div>
        <!-- <div>
            <hr>
            <div class='row text-center'>
                <div class='col'>
                    <p class='h3'>Overview of our Method</p>
                </div>
            </div>
			
			<div class='row mt-3'>                
                <div class='col-md-12 col-sm-12 col-xs-12 mt-3'>
                    <img src="assets/overview.png" class="img-fluid" alt="overview">
                </div>  
			</div> 
			<div class='row mt-3'>        				
                <div class='col-md-12 col-sm-12 col-xs-12 mt-3'>
                    <p class="text-break">
                    Given a pair of images, a shared-weight Siamese encoder extracts feature maps. We compute a 4D correlation
					volume using the inner product of features, from which our model predicts the relative rotation (here, as distributions over Euler angles).
                    </p>
                </div>
            </div>

            <div class='row mt-3'>
                <div class='col-md-6 col-sm-6 col-xs-12 mt-3'>
                    <img src="assets/4dcrv.jpg" class="img-fluid" alt="4dcrv">
                </div>
                <div class='col-md-6 col-sm-6 col-xs-12 align-middle mt-3'>
                    <p class="text-break">
                    A 4D correlation volume is
					calculated from a pair of image feature maps. Given a feature vector
					from Image 1, we compute the dot product with all feature vectors
					in Image 2, and build up a 2D slice of size H x W. Combining
					all 2D slices over all feature vectors in Image 1, we obtain a 4D
					correlation volume of size H x W x H x W.  
					<br>
					Our correlation volumes
					are implicitly assigned a dual role which emerges
					through training on both overlapping and non-overlapping
					pairs. When the input image pair contains significant overlap,
					pointwise correspondence can be computed and transferred
					onward to the rotation prediction module. When the input image
					pair contains little to no overlap, the correlation volume
					can assume the novel role of detecting implicit cues.
                    </p>
                </div>
            </div>
						
        </div> -->
    </body>
</html>
